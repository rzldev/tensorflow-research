## Building Model 1: Feed-Forward Neural Network (dense model) ##import pandas as pdimport random as randimport tensorflow as tfimport my_utils as utilsprint(f'\ntensorflow: {tf.__version__}')DF_PATH = '../data/nlp_getting_started'SAVE_DIR = './tmp/model_logs'MAX_VOCAB_LENGTH = 10000 # Max number of words to have in our vocabulariesMAX_LENGTH = 15 # Max length of our sequences will be (e.g. How many words from Tweet does a model see?)def run():    '''    Execute the code.    '''        ## Importing the dataset    training_set = pd.read_csv(f'{DF_PATH}/train.csv')    test_set = pd.read_csv(f'{DF_PATH}/test.csv')    print('\nTraining set:\n', training_set.head())    print('\nTest set:\n', test_set.head())        # Splitting the dataset into the training set and test set    from sklearn.model_selection import train_test_split        training_texts, validation_texts, training_labels, validation_labels = train_test_split(training_set.text.values,                                                                                            training_set.target.values,                                                                                            test_size=.2,                                                                                            random_state=42)        # Visualizing the data    print('\nVisualizing some random data: ')    for _ in range(4):        rand_idx = rand.randint(0, len(training_texts))        text = training_texts[rand_idx]        label = training_labels[rand_idx]        print(f'Target: {label}', '(diaster)' if label == 1 else '(not real diaster)')        print(f'Text:\n{text}')        print('\n---\n')        ## Text vectorization    from tensorflow.keras.layers import TextVectorization        # Use the default TextVectorization parameters    text_vectorizer = TextVectorization(max_tokens=MAX_VOCAB_LENGTH,                                        output_mode='int',                                        output_sequence_length=MAX_LENGTH)        # Fit the text vectorizer to the training set    text_vectorizer.adapt(training_texts)        # Choose a random text from training dataset and tokenize it    random_text = rand.choice(training_texts)    print(f'\nOriginal text: \n{random_text}\          \n\nVectorized version: \n', text_vectorizer([random_text]))        # Get the unique words in the vocabulary    words_in_vocab = text_vectorizer.get_vocabulary() # Get all the unique words in our vocabulary    top_5_words = words_in_vocab[:5]    bottom_5_words = words_in_vocab[-5:]    print(f'\nNumber of words in vocabulary: {len(words_in_vocab)}')    print(f'5 most common words: {top_5_words}')    print(f'5 least common words: {len(bottom_5_words)}')        ## Creating an Embedding using Embedding Layer    from tensorflow.keras.layers import Embedding        embedding = Embedding(input_dim=MAX_VOCAB_LENGTH, # Set input shape                          output_dim=128, # Set output shape                          input_length=MAX_LENGTH) # How long is each input        # Get a random text from the training set    random_text_2 = rand.choice(training_texts)    print(f'\nOriginal text: \n{random_text_2}')        # Embed the random text (turn it into dense vector of fixed size)    embedded_random_text_2 = embedding(text_vectorizer([random_text_2]))    print('\nEmbedded version: \n', embedded_random_text_2)        ## Create the Model 1: simple dense model    from tensorflow.keras import Model    from tensorflow.keras.layers import Input, GlobalAveragePooling1D, GlobalMaxPool1D, Dense    from tensorflow.keras.activations import sigmoid    from tensorflow.keras.optimizers import Adam    from tensorflow.keras.losses import BinaryCrossentropy        # Create the model with Functional API    inputs = Input(shape=(1,), dtype=tf.string) # Inputs are 1-dimensional strings    x = text_vectorizer(inputs) # Turn the input texts into numbers    x = embedding(x) # Create an embedding of the numberized inputs    x = GlobalAveragePooling1D()(x) # Condense the feature vector for each token to one vector    # x = GlobalMaxPool1D()(x)    outputs = Dense(1, activation=sigmoid)(x) # Create the output layer, want binary outputs so use sigmoid    model_1 = Model(inputs, outputs, name='model_1_simple_dense')        # Compile the model    model_1.compile(optimizer=Adam(),                    loss=BinaryCrossentropy(),                    metrics=['accuracy'])        # Model summary    print('\nModel Summary:')    model_1.summary()        # Fit the model    print(f'\nFitting {model_1.name}')    model_1_tensorboard_cb = utils.create_tensorboard_callback(dir_name=SAVE_DIR,                                                               experiment_name=model_1.name)    model_1_history = model_1.fit(x=training_texts, y=training_labels,                                  epochs=5,                                  validation_data=(validation_texts[:int(len(validation_texts)/2)],                                                    validation_labels[:int(len(validation_labels)/2)]),                                  callbacks=[model_1_tensorboard_cb])        ## Evaluating the model    print(f'\nEvaluating {model_1.name}')    model_1.evaluate(x=validation_texts, y=validation_labels)    utils.plot_model_history(model_1_history)        ## Making predictions & compare the results    print(f'\nMaking predictions using {model_1.name}')    model_1_preds = model_1.predict(validation_texts)    model_1_preds = tf.squeeze(tf.round(model_1_preds))    model_1_results = utils.evaluate_model_results(validation_labels, model_1_preds)    print(f'\nModel 1 (Simple Dense) results: \n{model_1_results}')    utils.plot_confusion_matrix(validation_labels, model_1_preds)        ## Visualizing learned embeddings    import io        # Get the weight matrix of embedding layer    # (These are the numerical presentations of each token in out training data, which have been learned for ~5 epochs)    embedded_weights = model_1.layers[2].get_weights()[0]    print(f'\nEmbedding layer weights: {embedded_weights.shape}\n{embedded_weights}')        '''    NOTE:    Now we've got the embedding matrix out model has learned to represent our tokens,    let's see how we can visualize it. To do so, TensorFlow has a handy tool called    projector: https://projector.tensorflow.org/        And TensorFlow also has an incredible guide on word embeddings themselves:    https://tensorflow.org/tutorials/text/word_embeddings    '''        out_v = io.open('./tmp/vectors.tsv', 'w', encoding='utf-8')    out_m = io.open('./tmp/metadata.tsv', 'w', encoding='utf-8')        for index, word in enumerate(words_in_vocab):        if index == 0:            continue        vec = embedded_weights[index]        out_v.write('\t'.join([str(x) for x in vec]) + '\n')        out_m.write(word + '\n')    out_v.close()    out_m.close()        '''    NOTE:    You can visualize the generated files above using https://projector.tensorflow.org    and clicking the `load` button on the left side.        Helpful resources:    *   Jay Alammar's visualized word2vec post: https://jalammar.github.io/illustrated-word2vec/    *   TensorFlow's Word Embeedings guide: https://tensorflow.org/tutorials/text/word_embeddings/    '''        return model_1_history, model_1_results    ## Run the fileif __name__ == '__main__':    run()