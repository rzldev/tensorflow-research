## Helpful Functions ##import numpy as npimport matplotlib.pyplot as pltimport datetimeimport itertoolsimport timefrom sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_supportfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpointdef plot_model_history(history):    '''    Returns a separate loss and accuracy curves for validation metrics    '''    model_name = history.model.name    print(f'\nPlotting {model_name} history')    loss = history.history['loss']    val_loss = history.history['val_loss']    accuracy = history.history['accuracy']    val_accuracy = history.history['val_accuracy']    epochs = range(len(history.history['loss']))        plt.plot(epochs, loss, label='training_loss')    plt.plot(epochs, val_loss, label='val_loss')    plt.title(f'{model_name} Loss Curve Plot')    plt.ylabel('Loss')    plt.xlabel('Epoch')    plt.legend()    plt.show()        plt.plot(epochs, accuracy, label='training_accuracy')    plt.plot(epochs, val_accuracy, label='val_accuracy')    plt.title(f'{model_name} Accuracy Curve Plot')    plt.xlabel('Epoch')    plt.ylabel('Accuracy')    plt.legend()    plt.show()    def plot_confusion_matrix(y_test, y_pred, classes=False,title='Confusion Matrix', figsize=(5, 5),                          label_size=11, title_size=14, text_size=12, savefig=False,                          save_path='./'):    '''    Plot confusion matrix.    '''    # Create the confusion matrix    cm = confusion_matrix(y_test, y_pred)    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # Normalize our confusion matrix    n_classes = cm.shape[0]        # Pretify it    fig, ax = plt.subplots(figsize=figsize)    # Create a matrix plot    cax = ax.matshow(cm, cmap=plt.cm.Blues)    fig.colorbar(cax)        # Create classes        if classes:        labels = classes    else:        labels = np.arange(cm.shape[0])        # Label the axes    ax.set(title=title, xlabel='Predicted Label', ylabel='True Label',           xticks=np.arange(n_classes), yticks=np.arange(n_classes),           xticklabels=labels, yticklabels=labels)        # Set x-axis labels to bottom    ax.xaxis.set_label_position('bottom')    ax.xaxis.tick_bottom()        # Adjust label size    ax.xaxis.label.set_size(label_size)    ax.yaxis.label.set_size(label_size)    ax.title.set_size(title_size)        # Change the plot x-labels vertically    plt.xticks(rotation=70)        # Set threshold for different colors    threshold = (cm.max() + cm.min()) / 2.        # Plot the text on each cell    for i, j in itertools.product(range(n_classes), range(cm.shape[1])):        plt.text(j, i, f'{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)',                 horizontalalignment='center', size=text_size,                 color='white' if cm[i, j] > threshold else 'black')        # Save the figure    if savefig:        fig.savefig(save_path + title + '.png')        plt.show()def evaluate_model_results(y_true, y_pred):    '''    Calculate model accuracy, precision, recall and f1 score of a binary classification.    Accuracy: Default metrics for classification problems. Not the best for imbalance    classes.    Precision: Higher precision leads to less false positives.    Recall: Higher recall leads to less false negatives.    F1-Score: Combination of precision and recall, usually a good overall metric    for a classification model.        For a deep overview of many different evaluation methods, see the sklearn docs:    https://scikit-learn.org/stable/modules/model_evaluation.html    '''    # Calculate model accuracy    model_accuracy = accuracy_score(y_true, y_pred)    # Calculate model precision, recall, f1-score using `weighted` average    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred,                                                                                 average='weighted')    model_results = {        'accuracy': float(format(model_accuracy, '.5f')),        'precision': float(format(model_precision, '.5f')),        'recall': float(format(model_recall, '.5f')),        'f1-score': float(format(model_f1, '.5f'))        }    return model_resultsdef create_tensorboard_callback(dir_name, experiment_name):    '''    Create and return a tensorboard callback which can be used while fitting the    model.    '''    log_path = dir_name + '/' + experiment_name + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')    tensorboard_callback = TensorBoard(log_dir=log_path)    print(f'\nSaving TensorBoard log files to: {log_path}')    return tensorboard_callbackdef create_model_checkpoint_callback(dir_name, experiment_name, save_weights_only=True,                                     save_best_only=False):    '''    Create and return a model checkpoint callback which can be used while fitting     the model.    '''    checkpoint_path = dir_name + '/' + experiment_name    model_checkpoint = ModelCheckpoint(filepath=checkpoint_path,                                       save_weights_only=save_weights_only,                                       save_best_only=save_best_only,                                       save_freq='epoch', # Save for every epoch                                       verbose=1)    return model_checkpointdef pred_timer(model, samples):    '''    Times how long a model takes to make predictions on the sample data.    '''    start_time = time.perf_counter()    model.predict(samples)    end_time = time.perf_counter()    total_time = end_time - start_time    time_per_pred = total_time / len(samples)    return total_time, time_per_pred