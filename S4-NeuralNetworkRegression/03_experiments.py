## Experiments on Neural Network Regression ##import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.keras import Sequential, layers, optimizers, lossesfrom tensorflow.keras.metrics import mean_absolute_error as maefrom tensorflow.keras.metrics import  mean_squared_error as mseprint('\ntensorflow:', tf.__version__)'''NOTE:This will be about running experiments to improve the model itself. Creating 3initial experiment models with this plan:1. model_1: 1 layer, trained with 100 epochs.2. model_2: 2 layer, trained with 100 epochs.3. model_3: 2 layer, trained with 500 epochs.'''## Create useful methodsdef create_model(X_train, y_train, model_layers=[], name=None, epochs=1,                  optimizer=optimizers.SGD(), loss=losses.mae, metrics=['mae']):    model = Sequential(name=name)    for layer in model_layers:        model.add(layer)    model.add(layers.Dense(1, name='output_layer'))    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)        print('\n{0} summary: '.format(name))    model.build()    model.summary()        print('\nFitting {0}'.format(name))    model.fit(X_train, y_train, epochs=epochs, verbose=0)        return modeldef compare_prediction_with_real_value(y_test, *y_preds):    comparison_table = pd.DataFrame(y_test, columns=['Ground Truth'])    for i, y_pred in enumerate(y_preds):        comparison_table['Model {0}'.format((i+1))] = y_pred    return comparison_table    def create_data_plot(X_train, y_train, X_test, y_test, y_pred=[],                      title='Data Plot'):    plt.figure(figsize=(10, 7))    plt.scatter(X_train, y_train, c='b', label='Training data')    plt.scatter(X_test, y_test, c='g', label='Test data')    if (len(y_pred) > 0):        plt.scatter(X_test, y_pred, c='r', label='Predicted data')    plt.title(title)    plt.legend()    plt.show()    def evaluate_model(model, X_test, y_test, y_pred):    print('\nEvaluate {0} on the test set.'.format(model.name))    model.evaluate(X_test, y_test)    squeezed_y_pred = tf.squeeze(y_pred)    print('\nMean Absolute Error evaluation: \n', mae(y_test, squeezed_y_pred))    print('\nMean Squared Error evaluation: \n', mse(y_test, squeezed_y_pred))## Create the datatf.random.set_seed(42)X = tf.range(-1750, 1750, 70)y = tf.range(-4500, 4500, 180)## Splitting the dataset into training set and test setfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(),                                                     test_size=.2,                                                     random_state=0)X_train, X_test = tf.constant(X_train), tf.constant(X_test)y_train, y_test = tf.constant(y_train), tf.constant(y_test)print('\nX_train size: {0} ({1}%)\n{2}'.format(X_train.shape[0],                                               (X_train.shape[0] / X.shape[0] * 100),                                               X_train[:10]))print('X_test size: {0} ({1}%)\n{2}'.format(X_test.shape[0],                                            (X_test.shape[0] / X.shape[0] * 100),                                            X_test[:10]))print('y_train size: {0} ({1}%)\n{2}'.format(y_train.shape[0],                                             (y_train.shape[0] / y.shape[0] * 100),                                             y_train[:10]))print('y_test size: {0} ({1}%)\n{2}'.format(y_test.shape[0],                                            (y_test.shape[0] / y.shape[0] * 100),                                            y_test[:10]))## Visualizing the datacreate_data_plot(X_train, y_train, X_test, y_test,                  title='Data Before Feature Scaling')## Feature Scalingfrom sklearn.preprocessing import StandardScalersc_X = StandardScaler()sc_y = StandardScaler()X_train = sc_X.fit_transform(tf.expand_dims(X_train, axis=-1))X_test = sc_X.transform(tf.expand_dims(X_test, axis=-1))y_train = sc_y.fit_transform(tf.expand_dims(y_train, axis=-1))y_test = sc_y.transform(tf.expand_dims(y_test, axis=-1))create_data_plot(X_train, y_train, X_test, y_test,                 title='Data After Feature Scaling')'''NOTE:Feature scaling is a method used to normalize the range of independent variables or features of data. Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization.'''## Creating the models# model_1model_1 = create_model(X_train, y_train, model_layers=[    layers.Dense(100, activation='relu', input_shape=[1], name='input_layer')    ], name='model_1', epochs=100)# model_2model_2 = create_model(X_train, y_train, model_layers=[    layers.Dense(100, activation='relu', input_shape=[1], name='input_layer'),    layers.Dense(100, activation='relu', name='hidden_layer_1'),    ], name='model_2', epochs=100)# model_3model_3 = create_model(X_train, y_train, model_layers=[    layers.Dense(100, activation='relu', input_shape=[1], name='input_layer'),    layers.Dense(100, activation='relu', name='hidden_layer_1'),    ], name='model_3', epochs=500)## Making predictions and visualizing it# model_1y_pred_1 = model_1.predict(X_test)y_pred_2 = model_2.predict(X_test)y_pred_3 = model_3.predict(X_test)comparison_table = compare_prediction_with_real_value(sc_y.inverse_transform(y_test),                                                      sc_y.inverse_transform(y_pred_1),                                                      sc_y.inverse_transform(y_pred_2),                                                      sc_y.inverse_transform(y_pred_3))print('\ncomparison_table:', comparison_table)create_data_plot()create_data_plot(X_train, y_train, X_test, y_test,                   y_pred_1, title='Prediction Plot Using model_1')create_data_plot(X_train, y_train, X_test, y_test,                   y_pred_2, title='Prediction Plot Using model_2')create_data_plot(X_train, y_train, X_test, y_test,                   y_pred_3, title='Prediction Plot Using model_3')## Evaluate the models# models_1evaluate_model(model_1, X_test, y_test, y_pred_1)# models_2evaluate_model(model_2, X_test, y_test, y_pred_2)# models_3evaluate_model(model_3, X_test, y_test, y_pred_3)'''NOTE:One of the main goals should be to minimize the time between your experiments.The more experiments we do, the more things we'll figure out which do not work or inturn, get closer to figuring out what does work.TRACKING OUR EXPERIMENTS:One really good habit in machine learning modelling is to track the results of ourexperiments. These are some resources that can help us tracking our experiments:1.  TensorBoard    A component of the TensorFlow library to help track modelling experiments.2.  Weights & Biases    A tool for tracking all kinds of machine learning experiments (plugs straight    into TensorBoard).'''